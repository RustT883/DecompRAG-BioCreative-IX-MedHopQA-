{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9239f42e",
   "metadata": {},
   "source": [
    "# Biocreative IX Task 1: MedHopQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb86a2",
   "metadata": {},
   "source": [
    "![title](IBMC_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1771d3",
   "metadata": {},
   "source": [
    "## Team Info\n",
    "\n",
    "Team Name: ***Orekhovichi***\n",
    "\n",
    "- Rustam R. Taktashov, IBMC\n",
    "- Nadezhda Yu. Bizyukova, IBMC\n",
    "- Olga A. Tarasova, IBMC\n",
    "- Alexander V. Dmitriev, IBMC\n",
    "\n",
    "## Description\n",
    "This is a demonstration of the tool used for MedHopQA task. Make sure to follow the repository README file to install the required packages and setup the virtual environment\n",
    "\n",
    "Some of the notebook cells are interactive. If you don't want it, use separate .py files in the repo directory (**duplicates.py** and **decomp_inference.py**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c867f8",
   "metadata": {},
   "source": [
    "## Project Pipeline\n",
    "\n",
    "<img src=\"Diagram.png\" width=\"400\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264cf7bd",
   "metadata": {},
   "source": [
    "# Setting up a vector store\n",
    "\n",
    "## Step 1: Get the wikipedia dump\n",
    "\n",
    "Download the wikipedia dump in **xml.bz2** format. Make sure you have enough memory since the archived file is around 20 gigabytes in size. We used the 20250420 dump, but you can get a more recent one\n",
    "\n",
    "**Download Link**\n",
    "https://dumps.wikimedia.org/enwiki/20250420/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72d844",
   "metadata": {},
   "source": [
    "## Step 2: Extract the relevant articles\n",
    "\n",
    "In our case, we used direct category matching from the prepared **medical_categories.txt**. To create the list of categories, you can either: \\\n",
    "a) Parse the wikipedia [wikipedia tree](https://en.wikipedia.org/wiki/Special:CategoryTree) using wikipedia API \\\n",
    "b) Use a [PetScan](https://petscan.wmcloud.org/) tool \n",
    "\n",
    "The code block below follows the option a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e50acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def get_all_subcategories(\n",
    "    root_category=\"Medicine\",\n",
    "    max_depth=5,\n",
    "    checkpoint_interval=20,\n",
    "    output_file=\"./categories/medical_categories.txt\",\n",
    "    resume=True\n",
    "):\n",
    "    \"\"\"Fetch all subcategories recursively, with checkpoints.\"\"\"\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    session = requests.Session()\n",
    "    visited = set()\n",
    "    queue = [(root_category, 0)]\n",
    "    all_categories = set()\n",
    "    checkpoint_count = 0\n",
    "\n",
    "    # Resume from existing file\n",
    "    if resume:\n",
    "        try:\n",
    "            with open(output_file, \"r\") as f:\n",
    "                existing_cats = {line.strip() for line in f}\n",
    "                visited.update(existing_cats)\n",
    "                all_categories.update(existing_cats)\n",
    "                print(f\"Resumed with {len(existing_cats)} pre-loaded categories.\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    with tqdm(desc=\"Fetching subcategories\") as pbar, \\\n",
    "         open(output_file, \"a\" if resume else \"w\") as f_out:\n",
    "\n",
    "        while queue:\n",
    "            current_cat, depth = queue.pop(0)\n",
    "            if current_cat in visited or depth > max_depth:\n",
    "                continue\n",
    "            visited.add(current_cat)\n",
    "\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"list\": \"categorymembers\",\n",
    "                \"cmtitle\": f\"Category:{current_cat}\",\n",
    "                \"cmtype\": \"subcat\",\n",
    "                \"cmlimit\": \"500\",\n",
    "                \"format\": \"json\"\n",
    "            }\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    response = session.get(base_url, params=params).json()\n",
    "                    if \"query\" not in response:\n",
    "                        break\n",
    "\n",
    "                    for member in response[\"query\"][\"categorymembers\"]:\n",
    "                        subcat = member[\"title\"].replace(\"Category:\", \"\")\n",
    "                        if subcat not in visited:\n",
    "                            all_categories.add(subcat)\n",
    "                            queue.append((subcat, depth + 1))\n",
    "                            f_out.write(f\"{subcat}\\n\")\n",
    "                            f_out.flush()  # Ensure immediate write\n",
    "                            checkpoint_count += 1\n",
    "                            pbar.update(1)\n",
    "\n",
    "                            # Print checkpoint every N categories\n",
    "                            if checkpoint_count % checkpoint_interval == 0:\n",
    "                                print(f\"\\nCheckpoint ({checkpoint_count}): {subcat}\")\n",
    "\n",
    "                    # Pagination\n",
    "                    if \"continue\" in response:\n",
    "                        params[\"cmcontinue\"] = response[\"continue\"][\"cmcontinue\"]\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError fetching {current_cat}: {e}\")\n",
    "                    time.sleep(5)  # Rate limit protection\n",
    "                    break\n",
    "\n",
    "    return all_categories\n",
    "\n",
    "# Run\n",
    "health_subcats = get_all_subcategories(\n",
    "    root_category=\"Medicine\", #Change the category if you want\n",
    "    max_depth=4, # Be cautious when increasing or decreasing this value\n",
    "    checkpoint_interval=20,\n",
    "    output_file=\"./categories/surgery_categories.txt\",\n",
    "    resume=True\n",
    ")\n",
    "print(f\"\\nDone! Total categories: {len(health_subcats)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b38f4",
   "metadata": {},
   "source": [
    "The code block below is necessary to extract the articles in **JSONL** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7561fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import bz2\n",
    "from urllib.parse import quote\n",
    "import mwparserfromhell\n",
    "import mwxml\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "UNWANTED_SECTIONS = {\n",
    "    'references', 'notes', 'citations', 'sources',\n",
    "    'external links', 'bibliography',\n",
    "    'see also', 'footnotes', 'works cited'\n",
    "} #You can add or remove the unwanted sections\n",
    "\n",
    "WIKIPEDIA_BASE_URL = \"https://en.wikipedia.org/wiki/\"\n",
    "CHECKPOINT_INTERVAL = 100000  # Checkpoint at every 100000th article\n",
    "CHECKPOINT_FILE = \"checkpoint.json\"\n",
    "\n",
    "def load_health_categories(file_path: str) -> set[str]:\n",
    "    \"\"\"Load health categories from file, one per line, with case-insensitive matching.\"\"\"\n",
    "    categories = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Non-empty lines\n",
    "                normalized = line.lower().strip()\n",
    "                if normalized:\n",
    "                    categories.add(normalized)\n",
    "    return categories\n",
    "\n",
    "def get_wikipedia_url(title: str) -> str:\n",
    "    \"\"\"Generate Wikipedia URL from title.\"\"\"\n",
    "    return WIKIPEDIA_BASE_URL + quote(title.replace(\" \", \"_\"))\n",
    "\n",
    "def clean_wikitext(wikitext: str) -> str:\n",
    "    if not wikitext:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        parsed = mwparserfromhell.parse(wikitext)\n",
    "        \n",
    "        nodes_to_remove = []\n",
    "        for node in parsed.nodes:\n",
    "            if isinstance(node, mwparserfromhell.nodes.template.Template):\n",
    "                # Only remove infoboxes and similar templates, not all templates\n",
    "                if node.name.lower().strip().startswith(('infobox', 'cite', 'reflist')):\n",
    "                    nodes_to_remove.append(node)\n",
    "            elif isinstance(node, mwparserfromhell.nodes.tag.Tag) and node.tag.lower() == 'ref':\n",
    "                nodes_to_remove.append(node)\n",
    "            elif isinstance(node, mwparserfromhell.nodes.wikilink.Wikilink):\n",
    "                if node.title.lower().startswith(('file:', 'image:')):\n",
    "                    nodes_to_remove.append(node)\n",
    "        \n",
    "        for node in nodes_to_remove:\n",
    "            parsed.remove(node)\n",
    "        \n",
    "        text = parsed.strip_code()\n",
    "        \n",
    "        # Clean up\n",
    "        text = re.sub(r'\\[\\[([^|\\]]*?\\|)?([^\\]]*?)\\]\\]', r'\\2', text)  # Simplify links\n",
    "        text = re.sub(r'\\{\\{.*?\\}\\}', '', text)  # Remove templates\n",
    "        text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "        text = re.sub(r'(\\s\\.)+', '.', text)  # Fix spaced dots\n",
    "        \n",
    "        # Remove references section if present\n",
    "        lines = []\n",
    "        skip = False\n",
    "        for line in text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Check for section headers to skip unwanted sections\n",
    "            if line.startswith('=='):\n",
    "                section_match = re.match(r'=+\\s*(.*?)\\s*=+', line)\n",
    "                if section_match:\n",
    "                    current_section = section_match.group(1).lower()\n",
    "                    skip = current_section in UNWANTED_SECTIONS\n",
    "                    continue\n",
    "            \n",
    "            if not skip:\n",
    "                lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(lines).strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If parsing initial parsing fails\n",
    "        text = re.sub(r'\\{\\{(Infobox|infobox)[^\\}]*?\\}\\}', '', wikitext, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\[(File|Image):.*?\\]\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'<ref[^>]*>.*?</ref>', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'\\[\\[([^|\\]]*?\\|)?([^\\]]*?)\\]\\]', r'\\2', text)\n",
    "        text = re.sub(r'\\{\\{.*?\\}\\}', '', text)\n",
    "        text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "def save_checkpoint(data):\n",
    "    \"\"\"Save checkpoint data to disk.\"\"\"\n",
    "    temp_file = f\"{CHECKPOINT_FILE}.tmp\"\n",
    "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "    os.rename(temp_file, CHECKPOINT_FILE)\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load checkpoint data from disk if exists.\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "def process_dump(dump_path: str, output_file: str, health_categories: set[str]):\n",
    "    \"\"\"Process Wikipedia dump with category matching.\"\"\"\n",
    "    checkpoint = load_checkpoint()\n",
    "    if checkpoint:\n",
    "        processed_count = checkpoint['processed_count']\n",
    "        health_articles_found = checkpoint['health_articles_found']\n",
    "        print(f\"Resuming from checkpoint - previously processed {processed_count} articles\")\n",
    "    else:\n",
    "        processed_count = 0\n",
    "        health_articles_found = 0\n",
    "\n",
    "    output_dir = os.path.dirname(output_file) or \".\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with (bz2.open(dump_path) as dump_file,\n",
    "          open(output_file, 'a' if checkpoint else 'w', encoding='utf-8') as out_file):\n",
    "        \n",
    "        dump = mwxml.Dump.from_file(dump_file)\n",
    "        \n",
    "        for page in tqdm(dump, initial=processed_count, desc=\"Processing articles\"):\n",
    "            processed_count += 1\n",
    "            if not isinstance(page, mwxml.Page):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                revision = next(iter(page))\n",
    "                text = revision.text or \"\"\n",
    "                categories = set()\n",
    "                for match in re.finditer(r\"\\[\\[Category:(.*?)(?:\\|.*?)?\\]\\]\", text, re.IGNORECASE):\n",
    "                    category = match.group(1).strip()\n",
    "                    if category:  # Non-empty categories\n",
    "                        categories.add(category.lower())\n",
    "                \n",
    "                # Debug output\n",
    "                if processed_count < 10:\n",
    "                    print(f\"\\nDebug - Article: {page.title}\")\n",
    "                    print(f\"Categories found: {categories}\")\n",
    "                    print(f\"Health categories sample: {list(health_categories)[:5]}\")\n",
    "                \n",
    "                # Check for any overlap between article categories and health categories\n",
    "                if categories & health_categories:\n",
    "                    clean_text = clean_wikitext(text)\n",
    "                    if len(clean_text.split()) >= 50:\n",
    "                        json.dump({\n",
    "                            \"title\": page.title,\n",
    "                            \"url\": get_wikipedia_url(page.title),\n",
    "                            \"text\": clean_text,\n",
    "                            \"categories\": list(categories),\n",
    "                        }, out_file, ensure_ascii=False)\n",
    "                        out_file.write(\"\\n\")\n",
    "                        health_articles_found += 1\n",
    "                        \n",
    "                        # Debug output for found articles\n",
    "                        if health_articles_found < 5:\n",
    "                            print(f\"\\nFound medical article: {page.title}\")\n",
    "                            print(f\"Matching categories: {categories & health_categories}\")\n",
    "\n",
    "                if processed_count % CHECKPOINT_INTERVAL == 0:\n",
    "                    save_checkpoint({\n",
    "                        'processed_count': processed_count,\n",
    "                        'health_articles_found': health_articles_found\n",
    "                    })\n",
    "                    print(f\"\\nCheckpoint saved - Processed: {processed_count}, Found: {health_articles_found}\")\n",
    "                    out_file.flush()\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "\n",
    "    return health_articles_found\n",
    "\n",
    "def main():\n",
    "    WIKIPEDIA_DUMP_PATH = \"enwiki-[YYYY-MM-DD]-pages-articles-multistream.xml.bz2\" #change the [YYYY-MM-DD] for the dump you downloaded\n",
    "    HEALTH_CATEGORIES_FILE = \"medical_categories.txt\"\n",
    "    OUTPUT_FILE = \"medical_articles.jsonl\"\n",
    "\n",
    "    print(f\"Loading health categories from {HEALTH_CATEGORIES_FILE}...\")\n",
    "    health_categories = load_health_categories(HEALTH_CATEGORIES_FILE)\n",
    "    \n",
    "    print(f\"Loaded {len(health_categories)} unique health categories (normalized to lowercase).\")\n",
    "    print(\"Sample of first 5 categories:\")\n",
    "    print(\"\\n\".join(sorted(health_categories)[:5]))\n",
    "\n",
    "    print(f\"\\nProcessing Wikipedia dump to {OUTPUT_FILE}...\")\n",
    "    total_health_articles = process_dump(WIKIPEDIA_DUMP_PATH, OUTPUT_FILE, health_categories)\n",
    "\n",
    "    print(f\"\\nDone! Found {total_health_articles} health articles in total.\")\n",
    "    print(f\"All articles saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de0920",
   "metadata": {},
   "source": [
    "We're not done yet, since the output file may contain duplicates. \n",
    "\n",
    "To remove the duplicates, you can either launch the cell below or launch **duplicates.py** from the directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93073f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def find_duplicates(filename: str, check_by: str = \"title\") -> dict:\n",
    "    \"\"\"\n",
    "    Check for duplicate articles in a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        filename: Path to the JSONL file\n",
    "        check_by: Field to check for duplicates (\"title\", \"id\", or \"content\")\n",
    "                  or \"full\" to compare entire JSON content\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with duplicate keys and their line numbers\n",
    "    \"\"\"\n",
    "    duplicates = defaultdict(list)\n",
    "    total_articles = 0\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            total_articles += 1\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                if check_by == \"full\":\n",
    "                    key = json.dumps(data, sort_keys=True)\n",
    "                else:\n",
    "                    key = data.get(check_by, None)\n",
    "                    if key is None:\n",
    "                        print(f\"Warning: Missing '{check_by}' field in line {line_number}\")\n",
    "                        continue\n",
    "                \n",
    "                duplicates[key].append(line_number)\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"! Invalid JSON on line {line_number}\")\n",
    "                continue\n",
    "\n",
    "    # Filter non-duplicates\n",
    "    duplicates = {k: v for k, v in duplicates.items() if len(v) > 1}\n",
    "    \n",
    "    print(f\"\\nAnalyzed {total_articles} articles\")\n",
    "    print(f\"Found {len(duplicates)} sets of duplicates\\n\")\n",
    "    \n",
    "    for i, (key, lines) in enumerate(duplicates.items(), 1):\n",
    "        print(f\"Duplicate set #{i}:\")\n",
    "        print(f\"Key: {key[:100]}{'...' if len(str(key)) > 100 else ''}\")\n",
    "        print(f\"Appears on lines: {', '.join(map(str, lines))}\\n\")\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "def remove_duplicates(\n",
    "    input_file: str,\n",
    "    output_file: Optional[str] = None,\n",
    "    check_by: str = \"title\",\n",
    "    keep: str = \"first\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Remove duplicate entries from a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input JSONL file\n",
    "        output_file: Path to save the deduplicated file (None to modify in-place)\n",
    "        check_by: Field to check for duplicates (\"title\", \"id\", \"content\", or \"full\")\n",
    "        keep: Which duplicate to keep (\"first\" or \"last\")\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = input_file + \".tmp\"\n",
    "    \n",
    "    seen_keys = set()\n",
    "    removed_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        lines = list(infile)  # Read all lines to allow keeping last occurrence\n",
    "        if keep == \"last\":\n",
    "            lines = reversed(lines)\n",
    "        \n",
    "        for line in lines:\n",
    "            total_count += 1\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                if check_by == \"full\":\n",
    "                    key = json.dumps(data, sort_keys=True)\n",
    "                else:\n",
    "                    key = data.get(check_by, None)\n",
    "                    if key is None:\n",
    "                        # Keep entries with missing key field\n",
    "                        outfile.write(line)\n",
    "                        continue\n",
    "                \n",
    "                if key not in seen_keys:\n",
    "                    seen_keys.add(key)\n",
    "                    outfile.write(line)\n",
    "                else:\n",
    "                    removed_count += 1\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"! Invalid JSON on line {total_count}, keeping as-is\")\n",
    "                outfile.write(line)\n",
    "    \n",
    "    # If we were working with reversed lines, reverse back\n",
    "    if keep == \"last\":\n",
    "        with open(output_file, 'r+', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            f.seek(0)\n",
    "            f.writelines(reversed(lines))\n",
    "            f.truncate()\n",
    "    \n",
    "    # Replace original file if no output file was specified\n",
    "    if output_file.endswith('.tmp'):\n",
    "        import os\n",
    "        os.replace(output_file, input_file)\n",
    "    \n",
    "    print(f\"\\nProcessed {total_count} entries\")\n",
    "    print(f\"Removed {removed_count} duplicates\")\n",
    "    print(f\"Kept {total_count - removed_count} unique entries\")\n",
    "\n",
    "def run_in_notebook():\n",
    "    \"\"\"Interactive version for Jupyter Notebook\"\"\"\n",
    "    # Create widgets\n",
    "    file_upload = widgets.FileUpload(description=\"Upload JSONL file\", multiple=False)\n",
    "    check_by = widgets.Dropdown(\n",
    "        options=['title', 'id', 'content', 'full'],\n",
    "        value='title',\n",
    "        description='Check by:'\n",
    "    )\n",
    "    action = widgets.RadioButtons(\n",
    "        options=['Find duplicates', 'Remove duplicates'],\n",
    "        description='Action:'\n",
    "    )\n",
    "    keep = widgets.Dropdown(\n",
    "        options=['first', 'last'],\n",
    "        value='first',\n",
    "        description='Keep:'\n",
    "    )\n",
    "    output_file = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='output.jsonl (leave blank to overwrite)',\n",
    "        description='Output file:'\n",
    "    )\n",
    "    run_button = widgets.Button(description=\"Run\")\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Only show keep widget when removing duplicates\n",
    "    def update_widgets(change):\n",
    "        keep.layout.visibility = 'visible' if action.value == 'Remove duplicates' else 'hidden'\n",
    "        output_file.layout.visibility = 'visible' if action.value == 'Remove duplicates' else 'hidden'\n",
    "    \n",
    "    action.observe(update_widgets, names='value')\n",
    "    update_widgets(None)\n",
    "    \n",
    "    def on_run_button_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            if not file_upload.value:\n",
    "                print(\"Please upload a file first\")\n",
    "                return\n",
    "                \n",
    "            # Save uploaded file\n",
    "            filename = next(iter(file_upload.value))\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(file_upload.value[filename]['content'])\n",
    "            \n",
    "            if action.value == 'Find duplicates':\n",
    "                find_duplicates(filename, check_by=check_by.value)\n",
    "            else:\n",
    "                out_file = output_file.value if output_file.value else None\n",
    "                remove_duplicates(\n",
    "                    filename,\n",
    "                    output_file=out_file,\n",
    "                    check_by=check_by.value,\n",
    "                    keep=keep.value\n",
    "                )\n",
    "    \n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    \n",
    "    # Display the interface\n",
    "    display(widgets.VBox([\n",
    "        file_upload,\n",
    "        check_by,\n",
    "        action,\n",
    "        keep,\n",
    "        output_file,\n",
    "        run_button,\n",
    "        output\n",
    "    ]))\n",
    "\n",
    "\n",
    "run_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334e33f",
   "metadata": {},
   "source": [
    "## Step 3: Create vector stores\n",
    "\n",
    "### Chroma Vector Store\n",
    "\n",
    "We used a Langchain Chroma class to create vector stores. Make sure to use the chromadb version dependency which comes with installing lanchain-chroma.\n",
    "\n",
    "You can change the embedding function as you see fit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "import time\n",
    "import psutil\n",
    "from typing import List\n",
    "\n",
    "# ===== Configuration =====\n",
    "CHUNK_SIZE = 512 # In characters\n",
    "CHUNK_OVERLAP = 32 # In characters\n",
    "EMBEDDING_BATCH_SIZE = 10 # In chunks\n",
    "MAX_ARTICLE_LENGTH = 50000 # In characters\n",
    "MAX_CHUNKS_PER_ARTICLE = 100\n",
    "CHECKPOINT_INTERVAL = 50 # In chunks\n",
    "LOG_FILE = \"processing_log.txt\"\n",
    "PERSIST_DIRECTORY = \"./chroma_store\"\n",
    "EMBEDDING_MODEL_PATH = \"abhinand/MedEmbed-small-v0.1\" # Choose any embedding function you want from HuggingFace\n",
    "CHECKPOINT_FILE = \"processing_checkpoint.txt\"  # File to store the last processed line\n",
    "CHUNKED_INSERT_SIZE = 1000  # Number of documents to insert at once\n",
    "\n",
    "# ===== Memory Monitoring =====\n",
    "def memory_safe():\n",
    "    \"\"\"Check if we have sufficient memory to continue\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    if mem.available < 1 * 1024**3:  # Less than 1GB available\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        time.sleep(5)\n",
    "        mem = psutil.virtual_memory()\n",
    "        return mem.available >= 1.5 * 1024**3\n",
    "    return True\n",
    "\n",
    "# ===== GPU Configuration =====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# ===== Logging and Checkpointing =====\n",
    "def log(message):\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        f.write(f\"{time.ctime()}: {message}\\n\")\n",
    "    print(message)\n",
    "\n",
    "def save_checkpoint(line_number):\n",
    "    \"\"\"Save the current line number to resume from later\"\"\"\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        f.write(str(line_number))\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load the last processed line number, returns 0 if no checkpoint exists\"\"\"\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            return int(f.read().strip())\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        return 0\n",
    "\n",
    "# ===== ChromaDB Setup =====\n",
    "def get_embeddings():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_PATH,\n",
    "        model_kwargs={\"device\": device, \"trust_remote_code\": True},\n",
    "        encode_kwargs={\n",
    "            \"batch_size\": EMBEDDING_BATCH_SIZE,\n",
    "            \"normalize_embeddings\": True # You can change the encode_kwargs, specifically, the distance metrics. Refer to https://python.langchain.com/api_reference/chroma/index.html#module-langchain_chroma\n",
    "        }\n",
    "    )\n",
    "\n",
    "def split_list(splits, chunk_size):\n",
    "    \"\"\"Split a list into smaller chunks for memory-safe processing\"\"\"\n",
    "    for i in range(0, len(splits), chunk_size):\n",
    "        yield splits[i:i + chunk_size]\n",
    "\n",
    "# ===== Text Processing =====\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "def process_article(article: dict) -> List[Document]:\n",
    "    \"\"\"Process a single article and return Document objects\"\"\"\n",
    "    if not memory_safe():\n",
    "        raise MemoryError(\"Insufficient memory before processing article\")\n",
    "        \n",
    "    try:\n",
    "        text = article.get(\"text\", \"\")[:MAX_ARTICLE_LENGTH]\n",
    "        if not text.strip():\n",
    "            return []\n",
    "            \n",
    "        chunks = text_splitter.split_text(text)[:MAX_CHUNKS_PER_ARTICLE]\n",
    "        if not chunks:\n",
    "            return []\n",
    "            \n",
    "        # Prepare documents with metadata\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            categories = article.get(\"categories\", [])\n",
    "            categories_str = \"|\".join(str(cat) for cat in categories)[:1000]\n",
    "            documents.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"title\": article.get(\"title\", \"Untitled\")[:200],\n",
    "                    \"url\": article.get(\"url\", \"\"),\n",
    "                    \"categories\": categories_str\n",
    "                }\n",
    "            ))\n",
    "            \n",
    "        return documents\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"Article processing error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_file(file_path: str):\n",
    "    \"\"\"Main processing loop with robust error handling and checkpointing\"\"\"\n",
    "    embeddings = get_embeddings()\n",
    "    total_chunks = 0\n",
    "    processed_articles = 0\n",
    "    accumulated_docs = []\n",
    "    \n",
    "    # Load the last checkpoint\n",
    "    start_line = load_checkpoint()\n",
    "    if start_line > 0:\n",
    "        log(f\"Resuming from line {start_line}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for _ in range(start_line):\n",
    "                next(f)\n",
    "            \n",
    "            pbar = tqdm(total=os.path.getsize(file_path), desc=\"Processing articles\", unit='B', unit_scale=True)\n",
    "            pbar.update(f.tell())\n",
    "            \n",
    "            for line in f:\n",
    "                try:\n",
    "                    if not memory_safe():\n",
    "                        raise MemoryError(\"Insufficient system memory\")\n",
    "                        \n",
    "                    article = json.loads(line)\n",
    "                    documents = process_article(article)\n",
    "                    if documents:\n",
    "                        accumulated_docs.extend(documents)\n",
    "                        total_chunks += len(documents)\n",
    "                    processed_articles += 1\n",
    "                    \n",
    "                    current_line = start_line + processed_articles\n",
    "                    save_checkpoint(current_line)\n",
    "                    \n",
    "                    pbar.update(len(line.encode('utf-8')))\n",
    "                    \n",
    "                    # Process in chunks\n",
    "                    if len(accumulated_docs) >= CHUNKED_INSERT_SIZE:\n",
    "                        for docs_chunk in split_list(accumulated_docs, CHUNKED_INSERT_SIZE):\n",
    "                            _ = Chroma.from_documents(\n",
    "                                documents=docs_chunk,\n",
    "                                embedding=embeddings,\n",
    "                                persist_directory=PERSIST_DIRECTORY\n",
    "                            )\n",
    "                            torch.cuda.empty_cache()\n",
    "                            gc.collect()\n",
    "                        accumulated_docs = []\n",
    "                    \n",
    "                    if processed_articles % CHECKPOINT_INTERVAL == 0:\n",
    "                        log(f\"Checkpoint: Processed {current_line} lines, {total_chunks} chunks total\")\n",
    "                        gc.collect()\n",
    "                        if device == \"cuda\":\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    log(f\"JSON decode error at line {current_line}\")\n",
    "                    continue\n",
    "                except MemoryError as e:\n",
    "                    log(f\"Memory error at line {current_line}: {str(e)}\")\n",
    "                    time.sleep(30)  # Longer sleep for memory issues\n",
    "                    # Rewind the file pointer to retry the same line\n",
    "                    f.seek(pbar.n - len(line.encode('utf-8')))\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    log(f\"Error at line {current_line}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        # Process any remaining documents\n",
    "        if accumulated_docs:\n",
    "            for docs_chunk in split_list(accumulated_docs, CHUNKED_INSERT_SIZE):\n",
    "                _ = Chroma.from_documents(\n",
    "                    documents=docs_chunk,\n",
    "                    embedding=embeddings,\n",
    "                    persist_directory=PERSIST_DIRECTORY\n",
    "                )\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                    \n",
    "    except Exception as e:\n",
    "        log(f\"Fatal error: {str(e)}\")\n",
    "        raise  # Re-raise to exit the program\n",
    "    finally:\n",
    "        pbar.close()\n",
    "        log(f\"Processing completed up to line {start_line + processed_articles} with {total_chunks} chunks\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    log(f\"Starting processing on {device}\")\n",
    "    if device == \"cuda\":\n",
    "        log(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        log(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        process_file(\"medical_articles.jsonl\")\n",
    "        # If we complete successfully, remove the checkpoint file\n",
    "        if os.path.exists(CHECKPOINT_FILE):\n",
    "            os.remove(CHECKPOINT_FILE)\n",
    "        log(\"\\nProcessing completed successfully!\")\n",
    "    except Exception as e:\n",
    "        log(f\"\\nProcessing interrupted due to: {str(e)}\")\n",
    "        log(\"The program can be restarted to resume from the last checkpoint\")\n",
    "    \n",
    "    log(f\"Time elapsed: {(time.time() - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8b84f",
   "metadata": {},
   "source": [
    "### BM25S Store\n",
    "We use custom BM25S implementation and we also do store disk persistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_langchain.retrievers import BM25SRetriever \n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pprint import pprint\n",
    "\n",
    "def metadata_func(record: str, metadata: str) -> str:\n",
    "    metadata['title'] = record.get('title')\n",
    "    metadata['url'] = record.get('url')\n",
    "    metadata['categories'] = record.get('categories')\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"medical_articles.jsonl\",\n",
    "    jq_schema=\".\",\n",
    "    content_key=\"text\",\n",
    "    metadata_func=metadata_func,\n",
    "    json_lines=True\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "for doc in docs:\n",
    "    if 'source' in doc.metadata and 'seq_num' in doc.metadata:\n",
    "        del doc.metadata['source']\n",
    "        del doc.metadata['seq_num']\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, # In characters\n",
    "    chunk_overlap=32, # In characters\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Extract just the page_content from each Document to get a list of strings\n",
    "text_contents = [doc.page_content for doc in split_docs]\n",
    "metadata = [doc.metadata for doc in split_docs]\n",
    "\n",
    "retriever = BM25SRetriever.from_texts(text_contents, metadata, k=2, persist_directory='bm25s_store')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1312ab6",
   "metadata": {},
   "source": [
    "Done, we now have two stores. Next, during retrieval and generation steps, we can use 2 separate (dense and sparse) retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34624631",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fae5e6",
   "metadata": {},
   "source": [
    "You can either launch the cell below or launch **decomp_inference.py** from the repo directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from custom_langchain.retrievers import BM25SRetriever \n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from typing import Dict, Any\n",
    "import gc\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from time import sleep\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# ==== CONFIG ====\n",
    "device = torch.device(\"cuda\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "callbacks = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "# CSV output configuration\n",
    "CHECKPOINT_INTERVAL = 2\n",
    "RETRY_DELAY = 5 \n",
    "MAX_RETRIES = 3\n",
    "OUTPUT_CSV_PATH = \"./qa_results.csv\"\n",
    "\n",
    "# ==== DIRECTORIES ====\n",
    "core_embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"abhinand/MedEmbed-small-v0.1\", # Embedding function should be the same as the embedding function used to create vector store\n",
    "    model_kwargs={\n",
    "        'device': \"cpu\", \n",
    "        'trust_remote_code': True,\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        'normalize_embeddings': True,\n",
    "        'batch_size': 8 # Distance metric should be the same as the one used in vector store creation\n",
    "    }\n",
    ")\n",
    "\n",
    "persist_directory = \"./chroma_store\"\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=core_embeddings_model)\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url = \"http://localhost:11434\",\n",
    "    model=\"thewindmom/llama3-med42-8b\", \n",
    "    timeout=300, \n",
    "    temperature = 0.0,\n",
    "    disable_streaming = True,\n",
    "    num_ctx=8192\n",
    ")\n",
    "\n",
    "# ==== PROMPT TEMPLATES ====\n",
    "prompt_template = \"\"\"Context information is below.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "You are an expert in medicine, molecular biology and biochemistry. Answer the question below based strictly on the context above and using common sense. \n",
    "If the answer cannot be found in the context, say \"I couldn't find a definitive answer in my sources.\"\n",
    "For complex questions, break them down into logical sub-questions.\n",
    "Query: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "DECOMPOSITION_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Break down this medical question into simple, factual sub-questions that can be answered independently from medical literature and be used to answer the main question.\n",
    "Each sub-question should:\n",
    "1. Be answerable with a specific fact or short answer\n",
    "2. Build logically toward answering the main question\n",
    "3. Use clear medical terminology. Don't turn \"diagnostic procedure\" into \"test\"\n",
    "4. Duplicate the initial question in the numbered list of sub-questions\n",
    "5. Don't make the questions redundant\n",
    "6. Always make no more than 4 subquestions\n",
    "7. NEVER loose context (e.g. never make \"Which chromosome does this disorder primarily affect?\" for \"Which gene is associated with a rare genetic disorder characterized by bilateral congenital hearing loss and brain malformations?\", since you will loose \"brain malformations\" with \"bilateral congenital hearing loss\")\n",
    "8. NEVER make the sub-quesitons more complex than the initial question\n",
    "\n",
    "Output ONLY the sub-questions as a numbered list, nothing else.\n",
    "\n",
    "Question: {question}\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "\n",
    "COMPOSITION_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Combine these answers to sub-questions into a coherent final answer.\n",
    "Be precise and cite sources when available using the provided source references.\n",
    "\n",
    "Sub-question Answers:\n",
    "{intermediate_answers}\n",
    "\n",
    "Source References:\n",
    "{source_references}\n",
    "\n",
    "Original question: {main_question}\n",
    "\n",
    "ALWAYS Extract a concise SINGLE and SHORT final answer (1-3 word long) following these rules:\n",
    "1. Use complete formal names (e.g., \"Diabetes Mellitus, type 2\")\n",
    "2. For chromosome questions: format as \"Chromosome X\"\n",
    "3. For syndromes named after people: use full name (e.g., \"Carpenter's syndrome\")\n",
    "4. For yes/no: answer only \"Yes\" or \"No\"\n",
    "5. For true/false: answer only \"TRUE\" or \"FALSE\"\n",
    "6. Pick a single answer without writing synonyms (e.g., either write \"PID\" or \"Pelvic Inflammatory Disease\", NOT \"PID (Pelvic Inflammatory Disease)\")\n",
    "7. Don't overthink or overengineer the answers (e.g., write \"Cardiology\", NOT \"Adult Congenital Cardiology\")\n",
    "8. Make sure the answers are not recursive (e.g. for question \"What is the primary cause of the physical changes observed in males with Klinefelter syndrome during puberty?\" the answer should be not \"Klinefelter Syndrome\", but the cause of it, like \"X Chromosome\")\n",
    "9. For drug questions: don't mention drug form (e.g., \"Calamine\" but NOT \"Calamine lotion\")\n",
    "11. Answers like \"Indirectly\" or \"Probably\" are forbidden.\n",
    "10. If you don't know the answer, just write \"N/A\" and nothing else\n",
    "11. For protein questions: just give a protein name (e.g., \"Myelin\", but not \"Myelin protein\")\n",
    "\n",
    "Structure your final answer as follows:\n",
    "\n",
    "SHORT ANSWER: [your 1-3 word answer here]\n",
    "\n",
    "DETAILED ANSWER:\n",
    "[your detailed explanation here, citing sources like [1], [2] where appropriate]\n",
    "\n",
    "SOURCE REFERENCES:\n",
    "{source_references}\n",
    "\"\"\")\n",
    "\n",
    "# ==== HELPER FUNCTIONS ====\n",
    "def format_source_references(sources: list[dict]) -> str:\n",
    "    \"\"\"Format sources with numbering for citation in the answer\"\"\"\n",
    "    source_refs = []\n",
    "    for idx, source in enumerate(sources, 1):\n",
    "        if 'wikipedia.org' in source['source_url']:\n",
    "            source_refs.append(\n",
    "                f\"[{idx}] {source['source_title']} - {source['source_url']}\"\n",
    "            )\n",
    "    return \"\\n\".join(source_refs) if source_refs else \"\"\n",
    "\n",
    "def improve_retrieval(query: str, is_subquestion: bool = False, main_question: str = \"\") -> str:\n",
    "    \"\"\"Optimize the query for better retrieval while avoiding circular references\"\"\"\n",
    "    if is_subquestion and main_question:\n",
    "        optimization_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a medical question optimizer. Optimize this specific medical sub-question for document retrieval but never answer them (e.g. \"Which medical specialty is likely involved in the diagnosis and treatment of angiolipomas?\" is not \"Dermatology involvement in angiolipoma diagnosis and management\" but \"Angliolipoma medical specialty\")\n",
    "Focus only on the specific aspect asked about. Don't incorporate the main question.\n",
    "NEVER lose context (e.g. \"Which medical subspecialty primarily focuses on the diagnosis and management of skin lesions?\" is not \"Dermatology\" but \"Dermatology skin lesions medical specialty\")\n",
    "\n",
    "Don't explain your reasoning.\n",
    "\n",
    "Sub-question: {query}\n",
    "Optimized:\"\"\")\n",
    "    else:\n",
    "        optimization_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Optimize this medical question for better document retrieval. \n",
    "Don't explain your reasoning.\n",
    "\n",
    "Original: {query}\n",
    "Optimized:\"\"\")\n",
    "    \n",
    "    optimizer = optimization_prompt | llm | StrOutputParser()\n",
    "    return optimizer.invoke({\"query\": query})\n",
    "\n",
    "def format_document(doc: Document) -> str:\n",
    "    \"\"\"Custom document formatter that safely handles metadata\"\"\"\n",
    "    base_content = f\"Content: {doc.page_content}\\n\"\n",
    "    \n",
    "    if not hasattr(doc, 'metadata') or not doc.metadata:\n",
    "        return base_content\n",
    "    \n",
    "    metadata = doc.metadata\n",
    "    title = metadata.get('title', 'N/A')\n",
    "    url = metadata.get('url', 'N/A')\n",
    "    \n",
    "    return f\"Content: {doc.page_content[:500]}\\nSource: {doc.metadata.get('title','')}\\n\"\n",
    "\n",
    "def extract_short_answer(long_answer: str) -> str:\n",
    "    \"\"\"Extract the short answer from the long answer\"\"\"\n",
    "    # Look for the SHORT ANSWER: pattern\n",
    "    if \"SHORT ANSWER:\" in long_answer:\n",
    "        return long_answer.split(\"SHORT ANSWER:\")[1].split(\"\\n\")[0].strip()\n",
    "    # Fallback to first line that meets criteria\n",
    "    for line in long_answer.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('[') and len(line.split()) <= 3:\n",
    "            return line\n",
    "    return \"N/A\"\n",
    "\n",
    "def write_to_csv(qidx: str, question: str, short_answer: str, long_answer: str, file_path: str):\n",
    "    \"\"\"Write or append results to CSV file\"\"\"\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    \n",
    "    with open(file_path, mode='a', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['QIDX', 'Question', 'Short Answer', 'Long Answer']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        writer.writerow({\n",
    "            'QIDX': qidx,\n",
    "            'Question': question,\n",
    "            'Short Answer': short_answer,\n",
    "            'Long Answer': long_answer\n",
    "        })\n",
    "\n",
    "# ==== CHAIN SETUP ====\n",
    "class CustomStuffDocumentsChain(StuffDocumentsChain):\n",
    "    def _get_inputs(self, docs, **kwargs):\n",
    "        doc_strings = [format_document(doc) for doc in docs]\n",
    "        return {**{self.document_variable_name: \"\\n\\n\".join(doc_strings)}, **kwargs}\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT, callbacks=None, verbose=False)\n",
    "\n",
    "vectorstore_retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity\", # You can use different types of search, refer to https://python.langchain.com/api_reference/chroma/index.html#module-langchain_chroma\n",
    "    search_kwargs={\n",
    "        \"k\": 4 # You can use different numbers of top-k retrieved contexts\n",
    "    }\n",
    ")\n",
    "\n",
    "combine_documents_chain = CustomStuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "keyword_retriever = BM25SRetriever.from_persisted_directory(\"bm25s_store\", k=4)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vectorstore_retriever, keyword_retriever],\n",
    "    weights=[0.6, 0.4] # You can use different weights for each retriever\n",
    ")\n",
    "\n",
    "qa = RetrievalQA(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    retriever=ensemble_retriever,\n",
    "    verbose=False,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# ==== MAIN PROCESSING FUNCTIONS ====\n",
    "def decompose_question(question: str) -> List[str]:\n",
    "    \"\"\"Decomposes a complex question into sub-questions while keeping the original question\"\"\"\n",
    "    decomposition_chain = DECOMPOSITION_PROMPT | llm | StrOutputParser()\n",
    "    result = decomposition_chain.invoke({\"question\": question})\n",
    "    \n",
    "    sub_questions = []\n",
    "    for line in result.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and line[0].isdigit() and '. ' in line:\n",
    "            sub_questions.append(line.split('. ', 1)[1])\n",
    "    \n",
    "    if question not in sub_questions:\n",
    "        sub_questions.insert(0, question)\n",
    "    \n",
    "    return sub_questions\n",
    "\n",
    "def format_for_json(docs: list[Document]) -> list[dict]:\n",
    "    formatted = []\n",
    "    for doc in docs:\n",
    "        metadata = getattr(doc, 'metadata', {})\n",
    "        formatted.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"source_title\": metadata.get('title', 'N/A'),\n",
    "            \"source_url\": metadata.get('url', 'N/A')\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "def process_question(question: str, qidx: str = \"0\"):\n",
    "    try:\n",
    "        # Parallelize sub-question processing\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        \n",
    "        sub_questions = decompose_question(question)\n",
    "        print(f\"\\nDecomposed into sub-questions:\")\n",
    "        for i, q in enumerate(sub_questions, 1):\n",
    "            print(f\"{i}. {q}\")\n",
    "            \n",
    "        intermediate_answers = {}\n",
    "        all_sources = []\n",
    "        source_map = {} \n",
    "        \n",
    "        # Process questions in parallel\n",
    "        def process_subquestion(q, i):\n",
    "            is_subquestion = i > 0\n",
    "            optimized_query = improve_retrieval(q, is_subquestion, sub_questions[0])\n",
    "            print(f\"Optimized query: {optimized_query}\")\n",
    "            \n",
    "            qa_result = qa.invoke({\"query\": optimized_query[:512]})\n",
    "            return {\n",
    "                \"idx\": i,\n",
    "                \"question\": q,\n",
    "                \"optimized_query\": optimized_query,\n",
    "                \"answer\": qa_result['result'],\n",
    "                \"sources\": format_for_json(qa_result['source_documents'][:2])  # Reduced from 3\n",
    "            }\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = [executor.submit(process_subquestion, q, i) \n",
    "                      for i, q in enumerate(sub_questions)]\n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                i = result[\"idx\"]\n",
    "                intermediate_answers[f\"q{i}\"] = {\n",
    "                    \"question\": result[\"question\"],\n",
    "                    \"optimized_query\": result[\"optimized_query\"],\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"sources\": result[\"sources\"]\n",
    "                }\n",
    "                source_map[f\"q{i}\"] = result[\"sources\"]\n",
    "                all_sources.extend(result[\"sources\"])\n",
    "        \n",
    "        # Prepare source references\n",
    "        source_references = format_source_references(all_sources[:10])\n",
    "        \n",
    "        # Generate final answer\n",
    "        composition_chain = COMPOSITION_PROMPT | llm | StrOutputParser()\n",
    "        final_answer = composition_chain.invoke({\n",
    "            \"intermediate_answers\": \"\\n\".join(\n",
    "                f\"Q{i}: {v['question']}\\nA: {v['answer']}\" \n",
    "                for i, v in enumerate(intermediate_answers.values(), 1)\n",
    "            ),\n",
    "            \"source_references\": source_references,\n",
    "            \"main_question\": question\n",
    "        })\n",
    "        \n",
    "        # Ensure sources are included even if LLM didn't add them\n",
    "        if \"SOURCE REFERENCES:\" not in final_answer:\n",
    "            final_answer += f\"\\n\\nSOURCE REFERENCES:\\n{source_references}\"\n",
    "        \n",
    "        # Extract short answer\n",
    "        short_answer = extract_short_answer(final_answer)\n",
    "        \n",
    "        # Write to CSV\n",
    "        write_to_csv(qidx, question, short_answer, final_answer, OUTPUT_CSV_PATH)\n",
    "        \n",
    "        return {\n",
    "            \"Short_Answer\": short_answer,\n",
    "            \"Long_Answer\": final_answer,\n",
    "            \"Sources\": all_sources[:10],\n",
    "            \"Intermediate_steps\": intermediate_answers\n",
    "        }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during QA processing: {str(e)}\")\n",
    "        write_to_csv(qidx, question, f\"ERROR: {str(e)}\", \"\", OUTPUT_CSV_PATH)\n",
    "        return {\n",
    "            \"Short_Answer\": f\"ERROR: {str(e)}\",\n",
    "            \"Long_Answer\": \"\",\n",
    "            \"Sources\": [],\n",
    "            \"Intermediate_steps\": {}\n",
    "        }\n",
    "\n",
    "# ==== INTERACTIVE NOTEBOOK INTERFACE ====\n",
    "def notebook_interface():\n",
    "    \"\"\"Create an interactive interface for Jupyter Notebook\"\"\"\n",
    "    # Create widgets\n",
    "    mode = widgets.RadioButtons(\n",
    "        options=['Single Question', 'Batch from CSV'],\n",
    "        description='Mode:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    question_input = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Enter your medical question here...',\n",
    "        description='Question:',\n",
    "        disabled=False,\n",
    "        layout={'width': '80%', 'height': '100px'}\n",
    "    )\n",
    "    \n",
    "    file_upload = widgets.FileUpload(\n",
    "        description='Upload CSV',\n",
    "        multiple=False,\n",
    "        accept='.csv',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    output_csv = widgets.Text(\n",
    "        value=OUTPUT_CSV_PATH,\n",
    "        placeholder='output.csv',\n",
    "        description='Output CSV:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    run_button = widgets.Button(description=\"Run\")\n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    # Show/hide widgets based on mode\n",
    "    def update_widgets(change):\n",
    "        if change['new'] == 'Single Question':\n",
    "            question_input.layout.display = 'flex'\n",
    "            file_upload.layout.display = 'none'\n",
    "        else:\n",
    "            question_input.layout.display = 'none'\n",
    "            file_upload.layout.display = 'flex'\n",
    "    \n",
    "    mode.observe(update_widgets, names='value')\n",
    "    update_widgets({'new': mode.value})\n",
    "    \n",
    "    # Handle button click\n",
    "    def on_run_button_clicked(b):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            \n",
    "            if mode.value == 'Single Question':\n",
    "                if not question_input.value.strip():\n",
    "                    print(\"Please enter a question\")\n",
    "                    return\n",
    "                \n",
    "                print(\"Processing question...\")\n",
    "                result = process_question(question_input.value.strip(), \"1\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"FINAL ANSWER:\")\n",
    "                print(\"-\"*80)\n",
    "                print(result[\"Long_Answer\"])\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"INTERMEDIATE STEPS:\")\n",
    "                print(\"-\"*80)\n",
    "                for step in result.get(\"Intermediate_steps\", {}).values():\n",
    "                    print(f\"\\nQ: {step['question']}\")\n",
    "                    print(f\"A: {step['answer']}\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"SOURCE ATTRIBUTION:\")\n",
    "                print(\"-\"*80)\n",
    "                for i, source in enumerate(result.get(\"Sources\", [])[:5], 1):\n",
    "                    print(f\"\\n[{i}] {source.get('source_title', 'N/A')}\")\n",
    "                    print(f\"URL: {source.get('source_url', 'N/A')}\")\n",
    "                    print(f\"Content: {source.get('content', '')[:200]}...\")\n",
    "            \n",
    "            else:  # Batch mode\n",
    "                if not file_upload.value:\n",
    "                    print(\"Please upload a CSV file\")\n",
    "                    return\n",
    "                \n",
    "                # Save uploaded file\n",
    "                filename = next(iter(file_upload.value))\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(file_upload.value[filename]['content'])\n",
    "                \n",
    "                # Process questions\n",
    "                global OUTPUT_CSV_PATH\n",
    "                OUTPUT_CSV_PATH = output_csv.value if output_csv.value else OUTPUT_CSV_PATH\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_csv(filename)\n",
    "                    if len(df) == 0:\n",
    "                        print(\"Input CSV is empty\")\n",
    "                        return\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading input CSV: {str(e)}\")\n",
    "                    return\n",
    "\n",
    "                # Initialize processed questions tracking\n",
    "                processed_qidx = set()\n",
    "                if os.path.exists(OUTPUT_CSV_PATH):\n",
    "                    try:\n",
    "                        existing_df = pd.read_csv(OUTPUT_CSV_PATH, usecols=['QIDX'])\n",
    "                        processed_qidx = set(existing_df['QIDX'].astype(str).unique())\n",
    "                        print(f\"Resuming processing with {len(processed_qidx)} already completed questions\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not read existing output file - {str(e)}\")\n",
    "\n",
    "                # Create list of unprocessed questions\n",
    "                unprocessed = []\n",
    "                for _, row in df.iterrows():\n",
    "                    qidx = str(row['QIDX']) if 'QIDX' in row else str(row.name)\n",
    "                    if qidx not in processed_qidx:\n",
    "                        unprocessed.append((qidx, row['Question']))\n",
    "\n",
    "                total_to_process = len(unprocessed)\n",
    "                if total_to_process == 0:\n",
    "                    print(\"No new questions to process\")\n",
    "                    return\n",
    "\n",
    "                print(f\"Starting sequential processing of {total_to_process} questions\")\n",
    "                \n",
    "                # Process in batches (but sequentially within each batch)\n",
    "                batch_size = 4\n",
    "                with tqdm(total=total_to_process, desc=\"Processing questions\") as pbar:\n",
    "                    for batch_start in range(0, total_to_process, batch_size):\n",
    "                        batch = unprocessed[batch_start:batch_start + batch_size]\n",
    "                        batch_results = []\n",
    "                        \n",
    "                        for qidx, question in batch:\n",
    "                            try:\n",
    "                                result = process_question(question, qidx)\n",
    "                                batch_results.append({\n",
    "                                    'QIDX': qidx,\n",
    "                                    'Question': question,\n",
    "                                    'Short Answer': result.get(\"Short_Answer\", \"N/A\"),\n",
    "                                    'Long Answer': result.get(\"Long_Answer\", \"\")\n",
    "                                })\n",
    "                                pbar.update(1)\n",
    "                            except Exception as e:\n",
    "                                print(f\"\\nError processing question {qidx}: {str(e)}\")\n",
    "                                batch_results.append({\n",
    "                                    'QIDX': qidx,\n",
    "                                    'Question': question,\n",
    "                                    'Short Answer': f\"ERROR: {str(e)}\",\n",
    "                                    'Long Answer': \"\"\n",
    "                                })\n",
    "                                pbar.update(1)\n",
    "\n",
    "                        # Save batch results\n",
    "                        if batch_results:\n",
    "                            pd.DataFrame(batch_results).to_csv(\n",
    "                                OUTPUT_CSV_PATH,\n",
    "                                mode='a',\n",
    "                                header=not os.path.exists(OUTPUT_CSV_PATH),\n",
    "                                index=False\n",
    "                            )\n",
    "                        \n",
    "                        # Memory management\n",
    "                        gc.collect()\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                print(f\"\\nProcessing complete. Results saved to {OUTPUT_CSV_PATH}\")\n",
    "    \n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    \n",
    "    # Display the interface\n",
    "    display(widgets.VBox([\n",
    "        mode,\n",
    "        question_input,\n",
    "        file_upload,\n",
    "        widgets.HBox([output_csv, run_button]),\n",
    "        output_area\n",
    "    ]))\n",
    "\n",
    "\n",
    "notebook_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
